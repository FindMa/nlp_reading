### Transformer

* [Transformer 一篇就够了（一）： Self-attenstion](https://zhuanlan.zhihu.com/p/345680792)

* [Transformer 一篇就够了（二）： Transformer中的Self-attenstion](https://zhuanlan.zhihu.com/p/347492368)

* [Transformer 一篇就够了（三）： Transformer的实现](https://zhuanlan.zhihu.com/p/347709112)

* [Transformer 超详细解读，一图胜千言](https://zhuanlan.zhihu.com/p/214119876)



### Performer

* [谷歌联手DeepMind提出Performer：用新方式重新思考注意力机制](https://zhuanlan.zhihu.com/p/268795896)



### Informer

* [Linformer" 拍了拍 "被吊打的Transformers 后浪们](https://zhuanlan.zhihu.com/p/149890569)



### MEMO

* [bAbI又屠榜？DeepMind新模型MEMO引入Transformer，模仿人脑推理表现抢眼！](https://zhuanlan.zhihu.com/p/111996470)



### Transformer-XL

* [谷歌、CMU 重磅论文：Transformer 升级版，评估速度提升超 1800 倍！](https://zhuanlan.zhihu.com/p/54770086)
* [谷歌开源超强语言模型 Transformer-XL，两大技术解决长文本问题](https://zhuanlan.zhihu.com/p/56027916)
* [CMU和谷歌联手放出XL号Transformer！提速1800倍 | 代码+预训练模型+超参数](https://zhuanlan.zhihu.com/p/54909623)



### Very Deep Transformers

* [github](https://github.com/namisan/exdeep-nmt1)
* [把Transformer加深几倍，会怎么样？](https://zhuanlan.zhihu.com/p/212401751)

